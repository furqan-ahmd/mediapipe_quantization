#!/usr/bin/env python3
"""
MediaPipe Face Landmarker Quantization using AI Edge Quantizer

This script uses Google's AI Edge Quantizer to properly quantize MediaPipe
models to INT8 while maintaining Float32 input/output.

Unlike standard TFLite converter, AI Edge Quantizer CAN work with .tflite files!

Requirements:
    pip3 install ai-edge-quantizer tensorflow numpy

Usage:
    python3 quantize_with_ai_edge.py

Output:
    face_landmarker_int8.task - Quantized model ready for Android
"""

import os
import sys
import zipfile
import numpy as np

print("="*70)
print("  MediaPipe Face Landmarker Quantization")
print("  Using AI Edge Quantizer")
print("="*70)
print()

# Check if ai_edge_quantizer is installed
try:
    import ai_edge_quantizer as aq
    print(f"‚úÖ AI Edge Quantizer version: {aq.__version__}")
except ImportError:
    print("‚ùå AI Edge Quantizer not installed!")
    print()
    print("Install with:")
    print("  pip3 install ai-edge-quantizer")
    print()
    sys.exit(1)

# Check TensorFlow
try:
    import tensorflow as tf
    print(f"‚úÖ TensorFlow version: {tf.__version__}")
except ImportError:
    print("‚ùå TensorFlow not installed!")
    print("  pip3 install tensorflow")
    sys.exit(1)

print()


def download_model(url, output_path):
    """Download MediaPipe model if not present."""
    if os.path.exists(output_path):
        print(f"‚úÖ Found existing {output_path}")
        return

    print(f"üì• Downloading {output_path}...")
    import urllib.request

    try:
        urllib.request.urlretrieve(url, output_path)
        size_mb = os.path.getsize(output_path) / 1024 / 1024
        print(f"‚úÖ Downloaded {output_path} ({size_mb:.1f} MB)")
    except Exception as e:
        print(f"‚ùå Download failed: {e}")
        sys.exit(1)


def extract_task_file(task_path, extract_dir):
    """Extract .tflite models from .task file (ZIP format)."""
    print(f"\nüì¶ Extracting {task_path}...")

    os.makedirs(extract_dir, exist_ok=True)

    with zipfile.ZipFile(task_path, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)

    files = [f for f in os.listdir(extract_dir) if f.endswith('.tflite')]
    print(f"‚úÖ Extracted {len(files)} .tflite models:")
    for f in files:
        size_kb = os.path.getsize(os.path.join(extract_dir, f)) / 1024
        print(f"   - {f} ({size_kb:.1f} KB)")

    return files


def quantize_dynamic_range(input_path, output_path):
    """
    Dynamic range quantization (recommended for GPU).

    Quantizes weights to INT8, keeps activations as Float32.
    No calibration data needed!
    """
    print(f"\nüîß Quantizing: {os.path.basename(input_path)}")
    print(f"   Method: Dynamic Range (W:INT8, A:FP32)")

    try:
        # Initialize quantizer with .tflite model
        quantizer = aq.Quantizer(input_path)

        # Apply dynamic range quantization recipe
        # This is the KEY: weights ‚Üí INT8, activations ‚Üí Float32
        quantizer.load_quantization_recipe(aq.recipe.dynamic_wi8_afp32())

        # Quantize and export
        quantized_result = quantizer.quantize()
        quantized_result.export_model(output_path)

        # Compare sizes
        original_size = os.path.getsize(input_path) / 1024
        quantized_size = os.path.getsize(output_path) / 1024
        reduction = (1 - quantized_size / original_size) * 100

        print(f"   ‚úÖ Original:  {original_size:.1f} KB")
        print(f"   ‚úÖ Quantized: {quantized_size:.1f} KB ({reduction:.1f}% smaller)")

        return True

    except Exception as e:
        print(f"   ‚ùå Quantization failed: {e}")
        return False


def repackage_task_file(quantized_dir, original_task, output_task):
    """
    Repackage quantized .tflite files back into .task file.

    Preserves the same structure as original .task file.
    """
    print(f"\nüì¶ Repackaging into {output_task}...")

    # Get list of models from original .task
    with zipfile.ZipFile(original_task, 'r') as original_zip:
        original_files = original_zip.namelist()

    # Create new .task with quantized models
    with zipfile.ZipFile(output_task, 'w', zipfile.ZIP_DEFLATED) as task_zip:
        for filename in original_files:
            quantized_path = os.path.join(quantized_dir, filename)

            if os.path.exists(quantized_path):
                # Use quantized version
                task_zip.write(quantized_path, filename)
                print(f"   ‚úÖ Added: {filename} (quantized)")
            else:
                # Copy original (non-.tflite files)
                with zipfile.ZipFile(original_task, 'r') as orig:
                    data = orig.read(filename)
                    task_zip.writestr(filename, data)
                    print(f"   ‚ÑπÔ∏è  Added: {filename} (original)")

    # Report final size
    original_size = os.path.getsize(original_task) / 1024 / 1024
    quantized_size = os.path.getsize(output_task) / 1024 / 1024
    reduction = (1 - quantized_size / original_size) * 100

    print(f"\nüìä Task File Comparison:")
    print(f"   Original:  {original_size:.2f} MB")
    print(f"   Quantized: {quantized_size:.2f} MB ({reduction:.1f}% smaller)")


def main():
    """Main quantization workflow."""

    # Configuration
    TASK_URL = "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task"
    INPUT_TASK = "face_landmarker.task"
    OUTPUT_TASK = "face_landmarker_int8.task"
    EXTRACT_DIR = "extracted"
    QUANTIZED_DIR = "quantized"

    # Step 1: Download model
    download_model(TASK_URL, INPUT_TASK)

    # Step 2: Extract .task file
    tflite_files = extract_task_file(INPUT_TASK, EXTRACT_DIR)

    # Step 3: Quantize each .tflite model
    os.makedirs(QUANTIZED_DIR, exist_ok=True)

    print("\n" + "="*70)
    print("  Quantizing Models")
    print("="*70)

    success_count = 0
    for tflite_file in tflite_files:
        input_path = os.path.join(EXTRACT_DIR, tflite_file)
        output_path = os.path.join(QUANTIZED_DIR, tflite_file)

        if quantize_dynamic_range(input_path, output_path):
            success_count += 1

    print(f"\n‚úÖ Successfully quantized {success_count}/{len(tflite_files)} models")

    # Step 4: Repackage into .task file
    if success_count > 0:
        repackage_task_file(QUANTIZED_DIR, INPUT_TASK, OUTPUT_TASK)

        print("\n" + "="*70)
        print("  üéâ SUCCESS!")
        print("="*70)
        print(f"\n‚úÖ Created: {OUTPUT_TASK}")
        print()
        print("üìù Next Steps:")
        print()
        print("1. Copy to Android app:")
        print(f"   cp {OUTPUT_TASK} \\")
        print(f"      nosmai_camera_sdk/src/android/java/nosmai/src/main/assets/")
        print()
        print("2. Update Java code:")
        print("   MediaPipeFaceLandmarkerBridge.java:")
        print('   modelPath = assetManager.open("face_landmarker_int8.task");')
        print()
        print("3. Rebuild and test!")
        print()
        print("üìä Expected Results:")
        print("   ‚Ä¢ Model size: ~50% smaller")
        print("   ‚Ä¢ Inference:  ~20-30% faster")
        print("   ‚Ä¢ Accuracy:   ~98-99% of original")
        print("   ‚Ä¢ Your code:  NO CHANGES NEEDED (Float32 I/O preserved)")
        print()
        print("‚ö° Pro Tip: Combine with these optimizations:")
        print("   ‚Ä¢ Disable blendshapes: .setOutputFaceBlendshapes(false)")
        print("   ‚Ä¢ Use LIVE_STREAM mode for better tracking")
        print("   ‚Ä¢ Total speedup: 40-50% faster!")
        print()

    else:
        print("\n‚ùå Quantization failed for all models")
        print()
        print("Common Issues:")
        print("  1. Unsupported operations in model")
        print("  2. Model already quantized")
        print("  3. AI Edge Quantizer version incompatibility")
        print()
        print("Fallback: Use custom minimal build instead")
        print("  ./build_int8_mediapipe.sh")


if __name__ == "__main__":
    main()
